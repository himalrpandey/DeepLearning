{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c4fa682f514945e18687f802ad54e78c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55c815e026e34bd3930d507fa8f2eab7",
              "IPY_MODEL_af40b1b59c9f4ba6895291a80ef53ac1",
              "IPY_MODEL_cfe31778880a4b9ca0b2c6ea7d8c9360"
            ],
            "layout": "IPY_MODEL_d54c0335c0794f48a3aa318f99598496"
          }
        },
        "55c815e026e34bd3930d507fa8f2eab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb3dc121a98d428296ed4a08ed64fa48",
            "placeholder": "​",
            "style": "IPY_MODEL_2a4365f69244431898885b756eceaa88",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "af40b1b59c9f4ba6895291a80ef53ac1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94eb98dffde5447f8b26a4a2645bd47e",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d6e040411b84283823aef656960e612",
            "value": 4
          }
        },
        "cfe31778880a4b9ca0b2c6ea7d8c9360": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e76f3c3f4e344969160256d5e4801a8",
            "placeholder": "​",
            "style": "IPY_MODEL_3a96eec9033a46e58f93cc22ff661bde",
            "value": " 4/4 [01:22&lt;00:00, 17.22s/it]"
          }
        },
        "d54c0335c0794f48a3aa318f99598496": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb3dc121a98d428296ed4a08ed64fa48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a4365f69244431898885b756eceaa88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94eb98dffde5447f8b26a4a2645bd47e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d6e040411b84283823aef656960e612": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e76f3c3f4e344969160256d5e4801a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a96eec9033a46e58f93cc22ff661bde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himalrpandey/DeepLearning/blob/main/Himal_AIT_RAG_Assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG + LLM Assessment"
      ],
      "metadata": {
        "id": "e1xCp6n57BB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to create a Retrieval-Augmented Generation (RAG) system using a Large Language Model (LLM). The RAG system should be able to retrieve relevant information from a knowledge base and generate coherent and informative responses to user queries."
      ],
      "metadata": {
        "id": "qmI3jBxK68-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps:\n",
        "\n",
        "1. Choose a domain and collect a suitable dataset of documents (at least 5 documents - PDFs or HTML pages) to serve as the knowledge base for your RAG system. Select one of the following topics:\n",
        "   * latest scientific papers from arxiv.org,\n",
        "   * fiction books released,\n",
        "   * legal documents or,\n",
        "   * social media posts.\n",
        "\n",
        "   Make sure that the documents are newer then the training dataset of the applied LLM. (20 points)\n",
        "\n",
        "2. Create three relevant prompts to the dataset, and one irrelevant prompt. (20 points)\n",
        "\n",
        "3. Load an LLM with at least 5B parameters. (10 points)\n",
        "\n",
        "4. Test the LLM with your prompts. The goal should be that without the collected dataset your model is unable to answer the question. If it gives you a good answer, select another question to answer and maybe a different dataset. (10 points)\n",
        "\n",
        "5. Create a LangChain-based RAG system by setting up a vector database from the documents. (20 points)\n",
        "\n",
        "6. Provide your three relevant and one irrelevant prompts to your RAG system. For the relevant prompts, your RAG system should return relevant answers, and for the irrelevant prompt, an empty answer. (20 points)\n"
      ],
      "metadata": {
        "id": "K3STgq_s6_mV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline, BitsAndBytesConfig\n",
        "from textwrap import fill\n",
        "from langchain.prompts import PromptTemplate\n",
        "import locale\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.vectorstores.utils import filter_complex_metadata # 'filter_complex_metadata' removes complex metadata that are not in str, int, float or bool format\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "import os\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "# you need to define your private User Access Token from Huggingface\n",
        "# to be able to access models with accepted licence\n",
        "HUGGINGFACE_UAT=\"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\"\n",
        "login(HUGGINGFACE_UAT)\n",
        "\n",
        "!wget https://arxiv.org/pdf/2403.14799\n",
        "!wget https://arxiv.org/pdf/2401.05343\n",
        "!wget https://arxiv.org/pdf/2306.09239\n",
        "!wget https://arxiv.org/pdf/2104.01469\n",
        "!wget https://arxiv.org/pdf/1809.06303"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mk2ae7prkLXD",
        "outputId": "cb464ba9-2c3a-41b8-d594-4c11624352d7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "--2024-05-08 22:24:44--  https://arxiv.org/pdf/2403.14799\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.67.42, 151.101.3.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 744054 (727K) [application/pdf]\n",
            "Saving to: ‘2403.14799.1’\n",
            "\n",
            "2403.14799.1        100%[===================>] 726.62K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-05-08 22:24:44 (23.8 MB/s) - ‘2403.14799.1’ saved [744054/744054]\n",
            "\n",
            "--2024-05-08 22:24:44--  https://arxiv.org/pdf/2401.05343\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.67.42, 151.101.3.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9740387 (9.3M) [application/pdf]\n",
            "Saving to: ‘2401.05343.1’\n",
            "\n",
            "2401.05343.1        100%[===================>]   9.29M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-05-08 22:24:44 (126 MB/s) - ‘2401.05343.1’ saved [9740387/9740387]\n",
            "\n",
            "--2024-05-08 22:24:44--  https://arxiv.org/pdf/2306.09239\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.67.42, 151.101.3.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 584488 (571K) [application/pdf]\n",
            "Saving to: ‘2306.09239.1’\n",
            "\n",
            "2306.09239.1        100%[===================>] 570.79K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-05-08 22:24:44 (23.3 MB/s) - ‘2306.09239.1’ saved [584488/584488]\n",
            "\n",
            "--2024-05-08 22:24:44--  https://arxiv.org/pdf/2104.01469\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.67.42, 151.101.3.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 329476 (322K) [application/pdf]\n",
            "Saving to: ‘2104.01469.1’\n",
            "\n",
            "2104.01469.1        100%[===================>] 321.75K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-05-08 22:24:44 (12.0 MB/s) - ‘2104.01469.1’ saved [329476/329476]\n",
            "\n",
            "--2024-05-08 22:24:44--  https://arxiv.org/pdf/1809.06303\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.67.42, 151.101.3.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1643818 (1.6M) [application/pdf]\n",
            "Saving to: ‘1809.06303.1’\n",
            "\n",
            "1809.06303.1        100%[===================>]   1.57M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-05-08 22:24:44 (47.8 MB/s) - ‘1809.06303.1’ saved [1643818/1643818]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_prompt = \"Summarize the key findings on EEG complexity in ADHD diagnosis as discussed in recent 2024 studies\"\n",
        "second_prompt = \"Discuss the latest approaches in using topological data analysis for understanding brain signals in ADHD research.\"\n",
        "third_prompt = \"Evaluate the effectiveness of network structure exploitation in automatic identification of ADHD subjects according to recent studies.\"\n",
        "fourth_prompt_irrelevant = \"Provide a detailed guide on the cultivation and care of mint trees\""
      ],
      "metadata": {
        "id": "_DvlQTBD_i3P"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\" # 8B language model from Meta AI\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             quantization_config=quantization_config,\n",
        "                                             trust_remote_code=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "gen_cfg = GenerationConfig.from_pretrained(model_name)\n",
        "gen_cfg.max_new_tokens=512\n",
        "gen_cfg.temperature=0.0000001 # 0.0 # For RAG we would like to have determenistic answers\n",
        "gen_cfg.return_full_text=True\n",
        "gen_cfg.do_sample=True\n",
        "gen_cfg.repetition_penalty=1.11\n",
        "\n",
        "pipe=pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=gen_cfg\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230,
          "referenced_widgets": [
            "c4fa682f514945e18687f802ad54e78c",
            "55c815e026e34bd3930d507fa8f2eab7",
            "af40b1b59c9f4ba6895291a80ef53ac1",
            "cfe31778880a4b9ca0b2c6ea7d8c9360",
            "d54c0335c0794f48a3aa318f99598496",
            "bb3dc121a98d428296ed4a08ed64fa48",
            "2a4365f69244431898885b756eceaa88",
            "94eb98dffde5447f8b26a4a2645bd47e",
            "2d6e040411b84283823aef656960e612",
            "4e76f3c3f4e344969160256d5e4801a8",
            "3a96eec9033a46e58f93cc22ff661bde"
          ]
        },
        "id": "9Fg7wJaxC4Dc",
        "outputId": "544ea449-9dd6-4d8b-92b4-c68974cc16db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4fa682f514945e18687f802ad54e78c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")"
      ],
      "metadata": {
        "id": "G4oIcK82C6hK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [first_prompt, second_prompt, third_prompt, fourth_prompt_irrelevant]\n",
        "for prompt in prompts:\n",
        "  result = llm(prompt.format(text=prompt))\n",
        "  print(fill(result.strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGZkrFz3DRHN",
        "outputId": "0d50f02b-7533-48fc-eb83-cb2b15705923"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarize the key findings on EEG complexity in ADHD diagnosis as discussed in recent 2024\n",
            "studies.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9341239/) * [Discuss the potential\n",
            "implications of these findings for future research and clinical practice in ADHD diagnosis and\n",
            "treatment.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9341239/)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discuss the latest approaches in using topological data analysis for understanding brain signals in\n",
            "ADHD research. (30 minutes)   3. Present a case study on how machine learning and deep learning can\n",
            "be used to analyze EEG signals in ADHD diagnosis, highlighting the potential benefits and challenges\n",
            "of these approaches. (20 minutes)   4. Discuss the importance of considering individual differences\n",
            "and variability in brain function when developing diagnostic tools for ADHD, and present some recent\n",
            "advances in this area. (20 minutes)  The presentation will conclude with a summary of the key\n",
            "takeaways and future directions for the application of machine learning and topological data\n",
            "analysis in ADHD research.  ### References:  1. **Topological Data Analysis**:         * Carlsson et\n",
            "al. (2010). Topology and data. Notices of the American Mathematical Society, 57(10), 1322-1336.\n",
            "* Ghrist (2008). Barcodes: The persistent topology of data. Bulletin of the American Mathematical\n",
            "Society, 45(2), 161-187. 2. **Machine Learning and Deep Learning for EEG Signal Analysis**:\n",
            "* Lotte et al. (2016). A review of classification algorithms for EEG-based brain-computer\n",
            "interfaces. Journal of Neural Engineering, 13(5), 053001.         * Schirrmeister et al. (2020).\n",
            "Deep learning for brain-computer interfaces: A survey. IEEE Transactions on Neural Systems and\n",
            "Rehabilitation Engineering, 28(1), 11-24. 3. **ADHD Research and Diagnosis**:         * American\n",
            "Psychiatric Association. (2013). Diagnostic and statistical manual of mental disorders (5th ed.).\n",
            "Arlington, VA: Author.         * Nigg et al. (2017). Causal mechanisms of attention-\n",
            "deficit/hyperactivity disorder (ADHD) and their relations to comorbidity. Developmental Psychology,\n",
            "53(3), 531-544.  Please note that this is just an example outline, and you may choose to organize\n",
            "your presentation differently or include additional information. Make sure to keep your presentation\n",
            "concise and focused on the main points you want to convey. Good luck!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate the effectiveness of network structure exploitation in automatic identification of ADHD\n",
            "subjects according to recent studies. The results are presented as follows:  1. **Network-based\n",
            "feature extraction**: In a study published in 2020, researchers used functional magnetic resonance\n",
            "imaging (fMRI) data from 30 ADHD and 30 healthy control subjects to extract features from brain\n",
            "networks [14]. They found that the extracted features were significantly different between the two\n",
            "groups, with higher connectivity in the default mode network (DMN) and lower connectivity in the\n",
            "salience network (SN) in ADHD subjects. 2. **Graph theory analysis**: A study published in 2019\n",
            "analyzed the structural connectome of 20 ADHD and 20 healthy control subjects using diffusion tensor\n",
            "imaging (DTI) data [15]. The authors applied graph theory metrics to identify differences in network\n",
            "topology between the two groups. They found that ADHD subjects had reduced global efficiency and\n",
            "increased clustering coefficient compared to controls. 3. **Machine learning classification**:\n",
            "Researchers in 2018 used a combination of graph theory metrics and machine learning algorithms to\n",
            "classify 40 ADHD and 40 healthy control subjects based on their brain network characteristics [16].\n",
            "They achieved an accuracy of 85% in distinguishing between ADHD and control subjects.  These studies\n",
            "demonstrate the potential of network structure exploitation for identifying ADHD subjects. By\n",
            "analyzing brain network properties, researchers can uncover subtle differences in neural\n",
            "communication patterns that may be indicative of ADHD. However, it is essential to note that these\n",
            "findings should be replicated in larger, more diverse populations to ensure generalizability.  In\n",
            "conclusion, the application of network structure exploitation techniques has shown promise in\n",
            "automatically identifying ADHD subjects. Further research is needed to refine these methods and\n",
            "explore their potential in clinical settings.  References: [14] Zhang et al. (2020). Network-based\n",
            "feature extraction for ADHD diagnosis using fMRI data. NeuroImage: Clinical, 24, 102242. [15] Li et\n",
            "al. (2019). Graph theoretical analysis of the structural connectome in attention deficit\n",
            "hyperactivity disorder. Human Brain Mapping, 40(12), 4445-4456. [16] Wang et al. (2018). Machine\n",
            "learning classification of ADHD using graph theory metrics and resting-state fMRI data. Journal of\n",
            "Neural Engineering, 15(4), 046016.](https://www.ncbi.nlm.nih.gov/books/NBK544924/)  Please let me\n",
            "know if you would like me to elaborate or provide additional information!\n",
            "Provide a detailed guide on the cultivation and care of mint trees, including tips for pruning,\n",
            "watering, and pest control. Mint is a popular herb that can be grown in many parts of the world.\n",
            "It's known for its refreshing flavor and aroma, and it's often used in teas, cocktails, and\n",
            "desserts. If you're interested in growing your own mint tree, here are some steps to follow:  **Step\n",
            "1: Choose a Location**  Mint prefers partial shade to full sun, so choose a location that receives\n",
            "at least 4-6 hours of direct sunlight per day. Mint also likes well-draining soil, so avoid planting\n",
            "it in areas with standing water or where water tends to collect.  **Step 2: Plant the Seeds or\n",
            "Seedlings**  You can start mint from seeds or seedlings. Sow the seeds about 1/8 inch deep and 1-2\n",
            "inches apart in a pot filled with moistened soil. Keep the soil warm (around 70°F) and moist until\n",
            "germination, which should take around 7-10 days. Transplant the seedlings into individual pots once\n",
            "they have two sets of leaves.  Alternatively, you can purchase mint seedlings from a nursery or\n",
            "online retailer. Plant them in the ground or in containers, making sure the soil is moist but not\n",
            "waterlogged.  **Step 3: Water and Fertilize**  Water your mint plant regularly, keeping the soil\n",
            "consistently moist but not waterlogged. You can fertilize your mint plant with a balanced fertilizer\n",
            "during the growing season (spring-fall).  **Step 4: Prune and Shape**  Pruning your mint plant\n",
            "regularly will help keep it bushy and prevent it from becoming leggy. Pinch off the top set of\n",
            "leaves when the plant reaches about 6 inches tall to encourage branching. You can also prune back\n",
            "long stems to promote new growth.  To shape your mint plant, pinch off any weak or spindly growth,\n",
            "and trim back the stems to maintain a compact shape.  **Step 5: Monitor for Pests and Diseases**\n",
            "Keep an eye out for pests like aphids, whiteflies, and spider mites, which can infest mint plants.\n",
            "Use neem oil or insecticidal soap to control infestations.  Also, monitor your mint plant for signs\n",
            "of disease like yellowing leaves, black spots, or powdery mildew. Treat fungal diseases with\n",
            "fungicides, and remove infected leaves or stems to prevent the spread of disease.  **Tips and\n",
            "Varieties**  Here are some additional tips and varieties to consider:  * **Varieties:**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "chunked_pdf_doc = []\n",
        "loaders = [UnstructuredPDFLoader(fn) for fn in [\"/content/1809.06303v1.pdf\", \"/content/2104.01469v1.pdf\", \"/content/2306.09239v1.pdf\", \"/content/2401.05343v1.pdf\", \"/content/2403.14799v1.pdf\"]]\n",
        "\n",
        "for loader in loaders:\n",
        "    print(\"Loading raw document...\" + loader.file_path)\n",
        "    pdf_doc = loader.load()\n",
        "    updated_pdf_doc = filter_complex_metadata(pdf_doc)\n",
        "    print(\"Splitting text...\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=256)\n",
        "    documents = text_splitter.split_documents(updated_pdf_doc)\n",
        "    chunked_pdf_doc.extend(documents)\n",
        "\n",
        "len(chunked_pdf_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9uFumFoI7fI",
        "outputId": "43f625fd-493e-4fe9-f9f3-9a133fcdb23d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading raw document.../content/1809.06303v1.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/2104.01469v1.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/2306.09239v1.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/2401.05343v1.pdf\n",
            "Splitting text...\n",
            "Loading raw document.../content/2403.14799v1.pdf\n",
            "Splitting text...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "422"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "embeddings = HuggingFaceEmbeddings() # default model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
        "db_pdf = FAISS.from_documents(chunked_pdf_doc, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5lkY7ViJDts",
        "outputId": "d41f2658-a4a3-4acb-f7db-6f72681a1513"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 8.88 s, sys: 491 ms, total: 9.37 s\n",
            "Wall time: 12.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
      ],
      "metadata": {
        "id": "TW4sTYRNJaxU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "Chain_pdf = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db_pdf.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'k': 4, 'score_threshold': 0.2}),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "6wsJnHZDQjYT",
        "outputId": "a99246e9-b7e8-4f9b-d18c-e3786e224608"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for StuffDocumentsChain\n__root__\n  document_variable_name context was not found in llm_chain input_variables: ['text'] (type=value_error)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval_qa/base.py\u001b[0m in \u001b[0;36mfrom_chain_type\u001b[0;34m(cls, llm, chain_type, chain_type_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;34m\"\"\"Load chain from chain type.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0m_chain_type_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchain_type_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         combine_documents_chain = load_qa_chain(\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchain_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_chain_type_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/question_answering/__init__.py\u001b[0m in \u001b[0;36mload_qa_chain\u001b[0;34m(llm, chain_type, verbose, callback_manager, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;34mf\"Should be one of {loader_mapping.keys()}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[0;32m--> 249\u001b[0;31m     return loader_mapping[chain_type](\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/question_answering/__init__.py\u001b[0m in \u001b[0;36m_load_stuff_chain\u001b[0;34m(llm, prompt, document_variable_name, verbose, callback_manager, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     )\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# TODO: document prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     return StuffDocumentsChain(\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mllm_chain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mdocument_variable_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocument_variable_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mobject_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__dict__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for StuffDocumentsChain\n__root__\n  document_variable_name context was not found in llm_chain input_variables: ['text'] (type=value_error)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for query in prompts:\n",
        "  result = Chain_pdf.invoke(query)\n",
        "  print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "t084IIECJbL5",
        "outputId": "11e7c6b5-7022-410e-dba9-4ee5de871943"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Chain_pdf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-29c36567354a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChain_pdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Chain_pdf' is not defined"
          ]
        }
      ]
    }
  ]
}